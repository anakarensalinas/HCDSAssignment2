# HCDSAssignment2
Human Centered Data Science Assignment 2

Using the Perspective API, comments were analyzed and assigned a toxicity score. Through this project, I learned more about how comments are judged by this API, as well as practicing how to incorporate APIs into my projects. Keeping in line with my preconceived expectations of this API, comments that were detected to contain profanity (in the english language) were the ones marked the most toxic by both the manual reviewers (marked under the toxic label) and the API's toxicity score (marked under the 'score' label). Because the selected language was english, some foreign profane words were included in the non toxic label classification manually, although to my surprise some of them did indeed have high toxicity scores. In general, the average toxicity scores of the nontoxic labeled comments was closer to the overal mean of the scores.

The hypothesis on bias that I came up with centered on minority descriptors of race or ethnicity being marked by the API as being more toxic than the white descriptor. In order to try to test out that hypothesis, I added terms containing both white descriptors (an example would be "white historian") and minority descriptors of race or ethnicity (such as "black historian") that were then analyzed by the API and returned with a toxicity score. The biggest difference could be seen with the "black" descriptor in front for most cases, with the highest scores in the testing data being recorded for "black man" and "black people", with their scores (0.43249154, 0.4596299) going above the threshold for toxicity, which was determined using the average scores of the toxic and nontoxic comments from the existing labeled dataset. There was not as much of a drastic difference between white and the hispanic and asian descriptors, with these descriptors often sometimes being analyzed as having a lower toxic score.
